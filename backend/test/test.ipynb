{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44c99a6",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Lagent 受到 PyTorch 设计理念的启发。我们期望，与神经网络层的类比能让工作流程更清晰、更直观，这样用户只需专注于以 Python 风格创建层并定义它们之间的消息传递。这是一个简单的教程，能让你快速上手构建多智能体应用程序。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6a1c9",
   "metadata": {},
   "source": [
    "### Model as Agents \n",
    "\n",
    "Agents use AgentMessage for communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b34f6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspaces/zhujunmiao/demo/mysearch-llm/backend/.venv/lib/python3.12/site-packages/pyairports/airports.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 15:22:22 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='../models/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='../models/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=../models/Qwen2.5-1.5B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspaces/zhujunmiao/demo/mysearch-llm/backend/.venv/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:46: ResourceWarning: unclosed <socket.socket fd=90, family=2, type=2, proto=0, laddr=('192.168.0.11', 50590), raddr=('8.8.8.8', 80)>\n",
      "  get_ip(), get_open_port())\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[rank0]:[W623 15:22:23.495351839 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 15:22:23 model_runner.py:720] Starting to load model ../models/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc811b3d64f14a82a48213cb4ea616de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 15:22:24 model_runner.py:732] Loading model weights took 2.8875 GB\n",
      "INFO 06-23 15:22:25 gpu_executor.py:102] # GPU blocks: 37295, # CPU blocks: 9362\n",
      "INFO 06-23 15:22:27 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-23 15:22:27 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-23 15:22:36 model_runner.py:1225] Graph capturing finished in 9 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 29.44it/s, est. speed input: 1034.21 toks/s, output: 59.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='典' sender='Agent' formatted=None extra_info=None type=None receiver=None stream_state=<AgentStatusCode.END: 0>\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "from lagent.agents import Agent\n",
    "from lagent.schema import AgentMessage\n",
    "from lagent.llms import VllmModel, INTERNLM2_META\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "llm = VllmModel(\n",
    "    path=\"../models/Qwen2.5-1.5B-Instruct\",\n",
    "    meta_template=INTERNLM2_META,\n",
    "    tp=1,\n",
    "    top_k=1,\n",
    "    temperature=1.0,\n",
    "    stop_words=['<|im_end|>'],\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "system_prompt = '你的回答只能从“典”、“孝”、“急”三个字中选一个。'\n",
    "agent = Agent(llm, system_prompt)\n",
    "\n",
    "user_msg = AgentMessage(sender='user', content='今天天气情况')\n",
    "bot_msg = agent(user_msg)\n",
    "print(bot_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc782671",
   "metadata": {},
   "source": [
    "### Memory as State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939c7e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AgentMessage(content='今天天气情况', sender='user', formatted=None, extra_info=None, type=None, receiver=None, stream_state=<AgentStatusCode.END: 0>), AgentMessage(content='典', sender='Agent', formatted=None, extra_info=None, type=None, receiver=None, stream_state=<AgentStatusCode.END: 0>)]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[{'content': '今天天气情况', 'sender': 'user', 'formatted': None, 'extra_info': None, 'type': None, 'receiver': None, 'stream_state': <AgentStatusCode.END: 0>}, {'content': '典', 'sender': 'Agent', 'formatted': None, 'extra_info': None, 'type': None, 'receiver': None, 'stream_state': <AgentStatusCode.END: 0>}]\n"
     ]
    }
   ],
   "source": [
    "memory: List[AgentMessage] = agent.memory.get_memory()\n",
    "print(memory)\n",
    "print('-' * 120)\n",
    "dumped_memory: Dict[str, List[dict]] = agent.state_dict()\n",
    "print(dumped_memory['memory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd931f9",
   "metadata": {},
   "source": [
    "### 自定义消息聚合\n",
    "\n",
    "DefaultAggregator 在幕后被调用，用于将 AgentMessage 组装并转换为 OpenAI 消息格式。下面我们了实现一个简单的聚合器，它能够接收少样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union \n",
    "from lagent.memory import Memory \n",
    "from lagent.prompts import StrParser\n",
    "from lagent.agents.aggregator import DefaultAggregator\n",
    "\n",
    "class FewshotAggregator(DefaultAggregator):\n",
    "    def __init__(self, few_shot: List[dict] = None):\n",
    "        self.few_shot = few_shot or []\n",
    "\n",
    "    def aggregate(\n",
    "        self, \n",
    "        messages: Memory, \n",
    "        name: str, \n",
    "        parser: StrParser,\n",
    "        system_instruction: Union[str, dict, List[dict]] = None         \n",
    "    ) -> List[dict]:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853f441",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
